\section{Motivation}
\label{sec:int_motivation}



It is not always easy to understand and introduce new paradigms. Some things we find natural nowadays, were once the source of controversy. Taking for example the number zero $0$. As numbers were introduced to help count physical objects, the idea of representing ``nothingness'' was once considered strange. In the beginning there were numerous ways devised to deal with this mathematical inconvenience, a special case. However the need to use ``zero'' as a number in its own right lead to the popularization of its concept as a number and opened door to new breakthroughs in mathematics\cite{Kaplan2000}. 

Quantum mechanics shared this same problem and its mathematical formalization grew out of the need to explain phenomena at the atomic scale\cite{Mehra1982}. Nowadays we  accepted quantum mechanics as a tool, to analyse and predict behaviour at a microscopic level. However at our scale some quantum mechanics phenomena seem almost ``ridicule'' and paradoxical.

The Quantum Computing started attracting interest in the decade of $1980$, with the works of Yuri Manin and Richard Feynman. The objective was to create a computational machine that could use the entanglement and superposition of the wave function to perform calculations that are currently impossible with classic computers. A popular example of that kind of computation is the prime factorization, which is in the base for modern cryptography. 
However we must bear in mind that our current computers are in fact quantic at the microscopic level. 
Silicon, a semiconductor, in in the base of modern chips, and its properties arise from quantum mechanics (it is neither a pure conductor, not a isolator material). Silicon is used to construct transistors that are based on the concept of accurate measuring; in this process the heat production is unavoidable. It is the heat produced by the microchips that make them hard to integrate and scale, and despite the Moore's Law that predicts that the number of transistors double every year (in the industry), there are physical limits that cannot be surpassed. When the transistors become too small they will start behaving in a quantum way, introducing errors in our deterministic computations \cite{Laughlin2005}.







The idea of trying to develop in quantum computing arises from the desire to explore this paradigm. The standard curriculum of an undergraduate computer scientist is focused on the current computational paradigm, which has its roots in von Neumman Architecture\cite{neumann45edvac}. While the construction and the inner-workings of computational systems that rely on new paradigms may fall outside of the scope of computer science, understanding it from the point of view of Information Technology and pushing boundaries on model representation are areas where computer scientist might contribute.


